"""
Code review module for AI-powered code analysis.

Provides functionality to analyze diffs for code smells and issues,
integrated with the existing AI providers.
"""

from dataclasses import dataclass, field
from pathlib import Path
from typing import Optional, List
import re


@dataclass
class CodeIssue:
    """Represents a code issue found during review."""
    type: str  # "code_smell", "bug", "style", "performance"
    description: str
    suggestion: str = ""
    severity: str = "warning"  # "info", "warning", "error"


@dataclass
class CodeReviewResult:
    """Result of AI code review."""
    has_issues: bool
    issues: list[CodeIssue] = field(default_factory=list)
    summary: str = ""
    
    @property
    def max_severity(self) -> str:
        """Get the highest severity among issues."""
        if not self.issues:
            return "info"
        severities = {"info": 0, "warning": 1, "error": 2}
        max_sev = max(self.issues, key=lambda i: severities.get(i.severity, 0))
        return max_sev.severity
    
    def has_blocking_issues(self, threshold: str = "error") -> bool:
        """Check if there are issues at or above the threshold severity."""
        severities = {"info": 0, "warning": 1, "error": 2}
        threshold_val = severities.get(threshold, 2)
        return any(
            severities.get(i.severity, 0) >= threshold_val 
            for i in self.issues
        )


# Dedicated prompt for standalone code review (separate from commit generation)
CODE_REVIEW_PROMPT = """
You are a Principal Software Engineer and System Architect.
Your specialty is high-scale React architectures and Next.js (App Router) optimization.
You have zero tolerance for technical debt, "clever" hacks that break at scale, or violations of modern design patterns.

Objective: Perform a critical audit of the provided code diff.
Your goal is to identify bottlenecks, security risks, and maintenance "time bombs".

Audit Checklist:
- Component Architecture: Misplacement of 'use client'. Detect logic that should be handled by Server Components to reduce bundle size.
- State Management & Hooks: Identify stale closures, missing dependencies in useEffect/useCallback, and redundant state that causes re-render loops.
- TypeScript Integrity: Flag any usage of 'any', weak interfaces, and missing exhaustive checks in discriminated unions.
- Performance: Locate O(n^2) operations inside the render cycle and lack of memoization on expensive computational branches.
- Next.js Paradigms: Check for proper use of Server Actions, Suspense boundaries, and Caching strategies (tags/revalidation).

Tone: Direct, technical, and uncompromising. If the code is problematic, explain why from a memory and performance perspective.

CRITICAL OUTPUT FORMAT (required for parsing):
Each issue MUST follow this exact format:
- [TYPE] <file:line> <problem_description> | <specific_fix_suggestion>

TYPE must be one of: SMELL, BUG, STYLE, PERF, SECURITY

EXAMPLE OUTPUT:
- [BUG] useEffect.ts:42 Missing dependency 'userId' in useEffect, will cause stale closure | Add 'userId' to the dependency array: [userId, fetchData]
- [PERF] DataTable.tsx:128 O(n¬≤) filter inside map operation on large dataset | Move filter outside the map or use useMemo to cache the filtered result
- [SECURITY] api/auth.ts:15 SQL query built with string concatenation | Use parameterized queries: db.query('SELECT * FROM users WHERE id = ?', [userId])

REQUIREMENTS:
1. Be SPECIFIC: Include file names and line numbers from the diff
2. Quote the PROBLEMATIC CODE snippet when relevant
3. Provide ACTIONABLE suggestions, not vague advice
4. If the code is fine, respond with ONLY: OK

Do NOT include any commit message. Only provide the code review.
"""

# Legacy: Additional system prompt for combined code review (appended to existing commit prompt)
CODE_REVIEW_PROMPT_ADDON = """

Additionally, analyze the code for potential issues and include a brief review section 
at the end of your response in the following format:

---CODE_REVIEW---
[If there are code quality issues, list them here. If the code looks good, write "OK"]

CRITICAL: Format each issue EXACTLY as below (required for parsing):
- [TYPE] Description | Suggestion

Where TYPE must be one of: SMELL, BUG, STYLE, PERF, SECURITY

If no significant issues found, just write:
OK - Code looks clean.

Remember: The commit message comes FIRST, then the code review section.
"""

# Header for example prompt files (generated by seshat init)
EXAMPLE_PROMPT_HEADER = """<!--
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                    SESHAT CODE REVIEW - EXEMPLO                       ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë Este arquivo foi gerado automaticamente pelo 'seshat init'.           ‚ïë
‚ïë                                                                        ‚ïë
‚ïë ‚ÆëÔ∏è  IMPORTANTE: Este √© apenas um EXEMPLO!                              ‚ïë
‚ïë                                                                        ‚ïë
‚ïë Edite este arquivo para atender √†s necessidades do seu projeto:       ‚ïë
‚ïë - Ajuste o foco de an√°lise para sua stack                             ‚ïë
‚ïë - Adicione regras espec√≠ficas do seu time                             ‚ïë
‚ïë - Remova itens que n√£o se aplicam                                     ‚ïë
‚ïë                                                                        ‚ïë
‚ïë Voc√™ pode deletar este coment√°rio ap√≥s customizar.                    ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
-->

"""

# TypeScript/React prompt (current default)
TYPESCRIPT_PROMPT = """You are a Principal Software Engineer specialized in TypeScript/React.
Your specialty is high-scale React architectures and Next.js (App Router) optimization.

Audit Checklist:
- Component Architecture: 'use client' vs Server Components optimization
- State Management: stale closures, missing hook dependencies, re-render loops
- TypeScript: 'any' abuse, weak interfaces, missing exhaustive checks
- Performance: O(n¬≤) in render cycle, missing memoization
- Next.js: Server Actions, Suspense boundaries, Caching strategies

CRITICAL OUTPUT FORMAT:
- [TYPE] <file:line> <problem> | <fix>

TYPE: SMELL, BUG, STYLE, PERF, SECURITY
If OK: OK
"""

# Python prompt
PYTHON_PROMPT = """You are a Senior Python Developer specialized in modern Python (3.10+).
Your focus is on clean architecture, type safety, and performance.

Audit Checklist:
- Type Safety: Missing type hints, incorrect Optional usage, Any abuse
- Async Patterns: Blocking calls in async context, missing await
- Security: SQL injection, command injection, insecure deserialization
- Performance: N+1 queries, inefficient loops, missing caching
- Django/FastAPI: Missing permission checks, unvalidated input, improper serialization

CRITICAL OUTPUT FORMAT:
- [TYPE] <file:line> <problem> | <fix>

TYPE: SMELL, BUG, STYLE, PERF, SECURITY
If OK: OK
"""

# Generic fallback prompt
GENERIC_PROMPT = """You are a Senior Software Engineer performing a code review.

Audit Checklist:
- Code Smells: Duplicated code, long methods, unclear naming
- Bugs: Logic errors, edge cases, null/undefined handling
- Security: Injection vulnerabilities, improper input validation
- Performance: Inefficient algorithms, unnecessary operations
- Maintainability: Missing error handling, unclear code flow

CRITICAL OUTPUT FORMAT:
- [TYPE] <file:line> <problem> | <fix>

TYPE: SMELL, BUG, STYLE, PERF, SECURITY
If OK: OK
"""

# Default prompts by language
DEFAULT_PROMPTS = {
    "typescript": TYPESCRIPT_PROMPT,
    "python": PYTHON_PROMPT,
    "generic": GENERIC_PROMPT,
}

# Prompts with example header (for seshat init)
EXAMPLE_PROMPTS = {
    "typescript": EXAMPLE_PROMPT_HEADER + TYPESCRIPT_PROMPT,
    "python": EXAMPLE_PROMPT_HEADER + PYTHON_PROMPT,
    "generic": EXAMPLE_PROMPT_HEADER + GENERIC_PROMPT,
}

# Default extensions for code review by language
DEFAULT_REVIEW_EXTENSIONS = {
    "typescript": [".ts", ".tsx", ".js", ".jsx", ".mjs", ".cjs"],
    "python": [".py", ".pyi"],
    "generic": [
        ".ts", ".tsx", ".js", ".jsx", ".mjs", ".cjs",
        ".py", ".pyi",
        ".go", ".rs", ".java", ".kt", ".swift",
        ".c", ".cpp", ".h", ".hpp",
        ".rb", ".php",
    ],
}


def get_default_extensions(project_type: Optional[str] = None) -> List[str]:
    """Get default extensions for code review based on project type."""
    if project_type and project_type in DEFAULT_REVIEW_EXTENSIONS:
        return DEFAULT_REVIEW_EXTENSIONS[project_type]
    return DEFAULT_REVIEW_EXTENSIONS["generic"]


def filter_diff_by_extensions(
    diff: str,
    extensions: Optional[List[str]] = None,
    project_type: Optional[str] = None,
) -> str:
    """
    Filter a git diff to only include files with specified extensions.
    
    Args:
        diff: Full git diff string
        extensions: List of extensions to include (e.g., [".ts", ".tsx"])
        project_type: Project type for default extensions if none specified
        
    Returns:
        Filtered diff containing only files with matching extensions
    """
    if not diff:
        return diff
    
    # Get extensions to use
    if extensions:
        # Normalize extensions (ensure they start with .)
        exts = [e if e.startswith(".") else f".{e}" for e in extensions]
    else:
        exts = get_default_extensions(project_type)
    
    # Convert to lowercase for case-insensitive matching
    exts = [e.lower() for e in exts]
    
    # Split diff into file sections
    # Each section starts with "diff --git a/..."
    file_pattern = re.compile(r'^diff --git a/(.+?) b/(.+?)$', re.MULTILINE)
    
    # Find all file boundaries
    matches = list(file_pattern.finditer(diff))
    
    if not matches:
        return diff  # No valid diff sections found
    
    filtered_sections = []
    
    for i, match in enumerate(matches):
        file_path = match.group(2)  # Use the "b/..." path (new file)
        file_ext = Path(file_path).suffix.lower()
        
        # Check if extension matches
        if file_ext in exts:
            # Get the section content
            start = match.start()
            end = matches[i + 1].start() if i + 1 < len(matches) else len(diff)
            filtered_sections.append(diff[start:end])
    
    return "".join(filtered_sections)


def get_example_prompt_for_language(project_type: Optional[str] = None) -> str:
    """Get example prompt with header for a language (used by seshat init)."""
    if project_type and project_type in EXAMPLE_PROMPTS:
        return EXAMPLE_PROMPTS[project_type]
    return EXAMPLE_PROMPTS["generic"]


def load_custom_prompt(prompt_path: str, base_path: str = ".") -> Optional[str]:
    """
    Load custom prompt from file.
    
    Args:
        prompt_path: Path to the prompt file (relative or absolute)
        base_path: Base path for relative paths
        
    Returns:
        Prompt content or None if file doesn't exist
    """
    # Handle absolute paths
    path = Path(prompt_path)
    if not path.is_absolute():
        path = Path(base_path) / prompt_path
    
    if path.exists():
        try:
            content = path.read_text(encoding="utf-8")
            # Strip HTML comments (the example header)
            # The AI should only see the actual prompt
            import re
            content = re.sub(r'<!--[\s\S]*?-->', '', content).strip()
            return content
        except Exception:
            return None
    return None


def get_review_prompt(
    project_type: Optional[str] = None,
    custom_path: Optional[str] = None,
    base_path: str = ".",
) -> str:
    """
    Get the appropriate review prompt.
    
    Priority:
    1. Custom prompt file (if specified and exists)
    2. Default prompt for project_type
    3. Generic fallback
    
    Args:
        project_type: Project type (typescript, python)
        custom_path: Path to custom prompt file
        base_path: Base path for relative paths
        
    Returns:
        The prompt string to use
    """
    # 1. Try custom prompt
    if custom_path:
        custom = load_custom_prompt(custom_path, base_path)
        if custom:
            return custom
    
    # 2. Try default for project type
    if project_type and project_type in DEFAULT_PROMPTS:
        return DEFAULT_PROMPTS[project_type]
    
    # 3. Fallback to generic
    return DEFAULT_PROMPTS["generic"]

def parse_code_review_response(response: str) -> tuple[str, CodeReviewResult]:
    """
    Parse AI response that contains both commit message and code review.
    
    Args:
        response: Full AI response with commit message and optional review.
        
    Returns:
        Tuple of (commit_message, CodeReviewResult)
    """
    # Split on the code review marker
    marker = "---CODE_REVIEW---"
    
    if marker not in response:
        # No code review section, return original message
        return response.strip(), CodeReviewResult(has_issues=False)
    
    parts = response.split(marker, 1)
    commit_message = parts[0].strip()
    review_section = parts[1].strip() if len(parts) > 1 else ""
    
    # Parse the review section
    result = CodeReviewResult(has_issues=False)
    
    if not review_section or "OK" in review_section.upper()[:20]:
        result.summary = "Code looks clean."
        return commit_message, result
    
    # Parse issues
    issues = []
    type_mapping = {
        "SMELL": "code_smell",
        "BUG": "bug",
        "STYLE": "style",
        "PERF": "performance",
        "SECURITY": "security",
    }
    
    for line in review_section.split("\n"):
        line = line.strip()
        if not line or line.startswith("#"):
            continue
        
        # Try to parse issue format: - [TYPE] Description | Suggestion
        if line.startswith("-"):
            line = line[1:].strip()
        
        issue_type = "code_smell"
        severity = "warning"
        description = line
        suggestion = ""
        
        # Extract type
        for marker_type, mapped_type in type_mapping.items():
            if f"[{marker_type}]" in line.upper():
                issue_type = mapped_type
                # Remove the type marker
                description = line.upper().replace(f"[{marker_type}]", "").strip()
                description = line[line.upper().find("]") + 1:].strip() if "]" in line else line
                break
        
        # Set severity based on type
        if issue_type in ("bug", "security"):
            severity = "error"
        elif issue_type == "code_smell":
            severity = "warning"
        else:
            severity = "info"
        
        # Extract suggestion if present
        if "|" in description:
            parts = description.split("|", 1)
            description = parts[0].strip()
            suggestion = parts[1].strip()
        
        if description and len(description) > 3:
            issues.append(CodeIssue(
                type=issue_type,
                description=description,
                suggestion=suggestion,
                severity=severity,
            ))
    
    result.issues = issues
    result.has_issues = len(issues) > 0
    result.summary = f"Found {len(issues)} issue(s)" if issues else "Code looks clean."
    
    return commit_message, result


def format_review_for_display(result: CodeReviewResult, verbose: bool = False) -> str:
    """Format code review result for terminal display."""
    if not result.has_issues:
        return "‚Æë Code review: No issues found."
    
    lines = [f"‚Æë Code review: {result.summary}"]
    
    severity_icons = {
        "info": "‚ÑπÔ∏è",
        "warning": "‚ÆëÔ∏è",
        "error": "‚Æë",
    }
    
    for issue in result.issues:
        icon = severity_icons.get(issue.severity, "‚Ä¢")
        lines.append(f"   {icon} [{issue.type}] {issue.description}")
        if verbose and issue.suggestion:
            lines.append(f"      üí° {issue.suggestion}")
    
    return "\n".join(lines)


def get_code_review_prompt_addon() -> str:
    """Get the prompt addon for code review."""
    return CODE_REVIEW_PROMPT_ADDON


def get_code_review_prompt() -> str:
    """Get the dedicated prompt for standalone code review."""
    return CODE_REVIEW_PROMPT


def parse_standalone_review(response: str) -> CodeReviewResult:
    """
    Parse AI response from standalone code review (no commit message).
    
    Args:
        response: AI response containing only code review.
        
    Returns:
        CodeReviewResult
    """
    response = response.strip()
    
    # Check for OK response (no issues)
    if response.upper().startswith("OK") or response.upper() == "OK":
        return CodeReviewResult(has_issues=False, summary="Code looks clean.")
    
    # Parse issues
    issues = []
    type_mapping = {
        "SMELL": "code_smell",
        "BUG": "bug",
        "STYLE": "style",
        "PERF": "performance",
        "SECURITY": "security",
    }
    
    for line in response.split("\n"):
        line = line.strip()
        if not line or line.startswith("#"):
            continue
        
        # Try to parse issue format: - [TYPE] Description | Suggestion
        if line.startswith("-"):
            line = line[1:].strip()
        
        issue_type = "code_smell"
        severity = "warning"
        description = line
        suggestion = ""
        
        # Extract type
        for marker_type, mapped_type in type_mapping.items():
            if f"[{marker_type}]" in line.upper():
                issue_type = mapped_type
                # Remove the type marker
                description = line[line.upper().find("]") + 1:].strip() if "]" in line else line
                break
        
        # Set severity based on type
        if issue_type in ("bug", "security"):
            severity = "error"
        elif issue_type == "code_smell":
            severity = "warning"
        else:
            severity = "info"
        
        # Extract suggestion if present
        if "|" in description:
            parts = description.split("|", 1)
            description = parts[0].strip()
            suggestion = parts[1].strip()
        
        if description and len(description) > 3:
            issues.append(CodeIssue(
                type=issue_type,
                description=description,
                suggestion=suggestion,
                severity=severity,
            ))
    
    return CodeReviewResult(
        has_issues=len(issues) > 0,
        issues=issues,
        summary=f"Found {len(issues)} issue(s)" if issues else "Code looks clean.",
    )


TYPE_PREFIX_RE = re.compile(r"^\s*-?\s*\[[A-Z]+\]\s*")
QUOTED_FILE_REF_RE = re.compile(
    r"(?P<quote>`|\"|')(?P<path>[^\n]+?):\d+(?::\d+)?(?P=quote)"
)
START_FILE_REF_RE = re.compile(
    r"^(?P<path>(?:[A-Za-z]:[\\/])?[^:\n]*\S):\d+(?::\d+)?\b"
)
TOKEN_FILE_REF_RE = re.compile(
    r"(?P<path>(?:[A-Za-z]:[\\/])?[^\s:\n]+):\d+(?::\d+)?\b"
)


def _extract_issue_filename(description: str) -> str:
    text = TYPE_PREFIX_RE.sub("", description.strip())

    match = QUOTED_FILE_REF_RE.search(text)
    if match:
        return match.group("path").strip("`'\"()[]{}<>")

    match = START_FILE_REF_RE.match(text)
    if match:
        return match.group("path").strip("`'\"()[]{}<>")

    match = TOKEN_FILE_REF_RE.search(text)
    if match:
        return match.group("path").strip("`'\"()[]{}<>")

    return "unknown"


def save_review_to_log(
    result: CodeReviewResult,
    log_dir: str,
    provider: str,
) -> List[str]:
    """
    Save review issues to log files.
    
    Args:
        result: The code review result containing issues
        log_dir: Directory to save logs to
        provider: AI provider name

    Returns:
        List of created log file paths
    """
    if not result.has_issues:
        return []
        
    import datetime
    
    created_logs = []
    log_path = Path(log_dir)
    log_path.mkdir(parents=True, exist_ok=True)
    
    current_time = datetime.datetime.now()
    date_str = current_time.strftime("%Y-%m-%d")
    time_str = current_time.strftime("%H:%M") # Use colon for log content as requested
    timestamp_suffix = current_time.strftime("%Y-%m-%d_%H-%M") # Keep dash for filename safety
    
    # Group issues by file
    issues_by_file: dict[str, list[CodeIssue]] = {}
    for issue in result.issues:
        # Extract filename from description if possible, or use "unknown"
        # Description format usually: "file.py:10 ..."
        filename = _extract_issue_filename(issue.description)
        
        # Keep the raw filename as the key so the log header shows the real path.
        if filename not in issues_by_file:
            issues_by_file[filename] = []
        issues_by_file[filename].append(issue)
        
    unknown_issues = issues_by_file.pop("unknown", None)

    for filename, issues in issues_by_file.items():

        # Format filename: relative-path-do-arquivo + '_' + date('Y-m-d_H-i').log
        # Replace path separators and drive markers with dashes
        safe_filename = filename.replace("/", "-").replace("\\", "-").replace(":", "-")
        log_filename = f"{safe_filename}_{timestamp_suffix}.log"
        full_log_path = log_path / log_filename
        
        with open(full_log_path, "w", encoding="utf-8") as f:
            f.write(f"Nome do arquivo: {filename}\n")
            f.write(f"Data: {date_str} {time_str}\n") # Using requested format
            f.write(f"IA revisora: {provider}\n")
            f.write("Descri√ß√£o do apontamento:\n")
            
            for issue in issues:
                f.write(f"- [{issue.type.upper()}] {issue.description}\n")
                if issue.suggestion:
                    f.write(f"  Sugest√£o: {issue.suggestion}\n")
                f.write("\n")
                
        created_logs.append(str(full_log_path))

    if unknown_issues:
        log_filename = f"unknown_{timestamp_suffix}.log"
        full_log_path = log_path / log_filename

        with open(full_log_path, "w", encoding="utf-8") as f:
            f.write("Nome do arquivo: unknown\n")
            f.write(f"Data: {date_str} {time_str}\n") # Using requested format
            f.write(f"IA revisora: {provider}\n")
            f.write("Descri√ß√£o do apontamento:\n")

            for issue in unknown_issues:
                f.write(f"- [{issue.type.upper()}] {issue.description}\n")
                if issue.suggestion:
                    f.write(f"  Sugest√£o: {issue.suggestion}\n")
                f.write("\n")

        created_logs.append(str(full_log_path))
        
    return created_logs
