"""
Code review module for AI-powered code analysis.

Provides functionality to analyze diffs for code smells and issues,
integrated with the existing AI providers.
"""

from dataclasses import dataclass, field
from pathlib import Path
from typing import Optional


@dataclass
class CodeIssue:
    """Represents a code issue found during review."""
    type: str  # "code_smell", "bug", "style", "performance"
    description: str
    suggestion: str = ""
    severity: str = "warning"  # "info", "warning", "error"


@dataclass
class CodeReviewResult:
    """Result of AI code review."""
    has_issues: bool
    issues: list[CodeIssue] = field(default_factory=list)
    summary: str = ""
    
    @property
    def max_severity(self) -> str:
        """Get the highest severity among issues."""
        if not self.issues:
            return "info"
        severities = {"info": 0, "warning": 1, "error": 2}
        max_sev = max(self.issues, key=lambda i: severities.get(i.severity, 0))
        return max_sev.severity
    
    def has_blocking_issues(self, threshold: str = "error") -> bool:
        """Check if there are issues at or above the threshold severity."""
        severities = {"info": 0, "warning": 1, "error": 2}
        threshold_val = severities.get(threshold, 2)
        return any(
            severities.get(i.severity, 0) >= threshold_val 
            for i in self.issues
        )


# Dedicated prompt for standalone code review (separate from commit generation)
CODE_REVIEW_PROMPT = """
You are a Principal Software Engineer and System Architect.
Your specialty is high-scale React architectures and Next.js (App Router) optimization.
You have zero tolerance for technical debt, "clever" hacks that break at scale, or violations of modern design patterns.

Objective: Perform a critical audit of the provided code diff.
Your goal is to identify bottlenecks, security risks, and maintenance "time bombs".

Audit Checklist:
- Component Architecture: Misplacement of 'use client'. Detect logic that should be handled by Server Components to reduce bundle size.
- State Management & Hooks: Identify stale closures, missing dependencies in useEffect/useCallback, and redundant state that causes re-render loops.
- TypeScript Integrity: Flag any usage of 'any', weak interfaces, and missing exhaustive checks in discriminated unions.
- Performance: Locate O(n^2) operations inside the render cycle and lack of memoization on expensive computational branches.
- Next.js Paradigms: Check for proper use of Server Actions, Suspense boundaries, and Caching strategies (tags/revalidation).

Tone: Direct, technical, and uncompromising. If the code is problematic, explain why from a memory and performance perspective.

CRITICAL OUTPUT FORMAT (required for parsing):
Each issue MUST follow this exact format:
- [TYPE] <file:line> <problem_description> | <specific_fix_suggestion>

TYPE must be one of: SMELL, BUG, STYLE, PERF, SECURITY

EXAMPLE OUTPUT:
- [BUG] useEffect.ts:42 Missing dependency 'userId' in useEffect, will cause stale closure | Add 'userId' to the dependency array: [userId, fetchData]
- [PERF] DataTable.tsx:128 O(nÂ²) filter inside map operation on large dataset | Move filter outside the map or use useMemo to cache the filtered result
- [SECURITY] api/auth.ts:15 SQL query built with string concatenation | Use parameterized queries: db.query('SELECT * FROM users WHERE id = ?', [userId])

REQUIREMENTS:
1. Be SPECIFIC: Include file names and line numbers from the diff
2. Quote the PROBLEMATIC CODE snippet when relevant
3. Provide ACTIONABLE suggestions, not vague advice
4. If the code is fine, respond with ONLY: OK

Do NOT include any commit message. Only provide the code review.
"""

# Legacy: Additional system prompt for combined code review (appended to existing commit prompt)
CODE_REVIEW_PROMPT_ADDON = """

Additionally, analyze the code for potential issues and include a brief review section 
at the end of your response in the following format:

---CODE_REVIEW---
[If there are code quality issues, list them here. If the code looks good, write "OK"]

CRITICAL: Format each issue EXACTLY as below (required for parsing):
- [TYPE] Description | Suggestion

Where TYPE must be one of: SMELL, BUG, STYLE, PERF, SECURITY

If no significant issues found, just write:
OK - Code looks clean.

Remember: The commit message comes FIRST, then the code review section.
"""

# Header for example prompt files (generated by seshat init)
EXAMPLE_PROMPT_HEADER = """<!--
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    SESHAT CODE REVIEW - EXEMPLO                       â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ Este arquivo foi gerado automaticamente pelo 'seshat init'.           â•‘
â•‘                                                                        â•‘
â•‘ âš ï¸  IMPORTANTE: Este Ã© apenas um EXEMPLO!                              â•‘
â•‘                                                                        â•‘
â•‘ Edite este arquivo para atender Ã s necessidades do seu projeto:       â•‘
â•‘ - Ajuste o foco de anÃ¡lise para sua stack                             â•‘
â•‘ - Adicione regras especÃ­ficas do seu time                             â•‘
â•‘ - Remova itens que nÃ£o se aplicam                                     â•‘
â•‘                                                                        â•‘
â•‘ VocÃª pode deletar este comentÃ¡rio apÃ³s customizar.                    â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
-->

"""

# TypeScript/React prompt (current default)
TYPESCRIPT_PROMPT = """You are a Principal Software Engineer specialized in TypeScript/React.
Your specialty is high-scale React architectures and Next.js (App Router) optimization.

Audit Checklist:
- Component Architecture: 'use client' vs Server Components optimization
- State Management: stale closures, missing hook dependencies, re-render loops
- TypeScript: 'any' abuse, weak interfaces, missing exhaustive checks
- Performance: O(nÂ²) in render cycle, missing memoization
- Next.js: Server Actions, Suspense boundaries, Caching strategies

CRITICAL OUTPUT FORMAT:
- [TYPE] <file:line> <problem> | <fix>

TYPE: SMELL, BUG, STYLE, PERF, SECURITY
If OK: OK
"""

# Python prompt
PYTHON_PROMPT = """You are a Senior Python Developer specialized in modern Python (3.10+).
Your focus is on clean architecture, type safety, and performance.

Audit Checklist:
- Type Safety: Missing type hints, incorrect Optional usage, Any abuse
- Async Patterns: Blocking calls in async context, missing await
- Security: SQL injection, command injection, insecure deserialization
- Performance: N+1 queries, inefficient loops, missing caching
- Django/FastAPI: Missing permission checks, unvalidated input, improper serialization

CRITICAL OUTPUT FORMAT:
- [TYPE] <file:line> <problem> | <fix>

TYPE: SMELL, BUG, STYLE, PERF, SECURITY
If OK: OK
"""

# Generic fallback prompt
GENERIC_PROMPT = """You are a Senior Software Engineer performing a code review.

Audit Checklist:
- Code Smells: Duplicated code, long methods, unclear naming
- Bugs: Logic errors, edge cases, null/undefined handling
- Security: Injection vulnerabilities, improper input validation
- Performance: Inefficient algorithms, unnecessary operations
- Maintainability: Missing error handling, unclear code flow

CRITICAL OUTPUT FORMAT:
- [TYPE] <file:line> <problem> | <fix>

TYPE: SMELL, BUG, STYLE, PERF, SECURITY
If OK: OK
"""

# Default prompts by language
DEFAULT_PROMPTS = {
    "typescript": TYPESCRIPT_PROMPT,
    "python": PYTHON_PROMPT,
    "generic": GENERIC_PROMPT,
}

# Prompts with example header (for seshat init)
EXAMPLE_PROMPTS = {
    "typescript": EXAMPLE_PROMPT_HEADER + TYPESCRIPT_PROMPT,
    "python": EXAMPLE_PROMPT_HEADER + PYTHON_PROMPT,
    "generic": EXAMPLE_PROMPT_HEADER + GENERIC_PROMPT,
}


def get_example_prompt_for_language(project_type: Optional[str] = None) -> str:
    """Get example prompt with header for a language (used by seshat init)."""
    if project_type and project_type in EXAMPLE_PROMPTS:
        return EXAMPLE_PROMPTS[project_type]
    return EXAMPLE_PROMPTS["generic"]


def load_custom_prompt(prompt_path: str, base_path: str = ".") -> Optional[str]:
    """
    Load custom prompt from file.
    
    Args:
        prompt_path: Path to the prompt file (relative or absolute)
        base_path: Base path for relative paths
        
    Returns:
        Prompt content or None if file doesn't exist
    """
    # Handle absolute paths
    path = Path(prompt_path)
    if not path.is_absolute():
        path = Path(base_path) / prompt_path
    
    if path.exists():
        try:
            content = path.read_text(encoding="utf-8")
            # Strip HTML comments (the example header)
            # The AI should only see the actual prompt
            import re
            content = re.sub(r'<!--[\s\S]*?-->', '', content).strip()
            return content
        except Exception:
            return None
    return None


def get_review_prompt(
    project_type: Optional[str] = None,
    custom_path: Optional[str] = None,
    base_path: str = ".",
) -> str:
    """
    Get the appropriate review prompt.
    
    Priority:
    1. Custom prompt file (if specified and exists)
    2. Default prompt for project_type
    3. Generic fallback
    
    Args:
        project_type: Project type (typescript, python)
        custom_path: Path to custom prompt file
        base_path: Base path for relative paths
        
    Returns:
        The prompt string to use
    """
    # 1. Try custom prompt
    if custom_path:
        custom = load_custom_prompt(custom_path, base_path)
        if custom:
            return custom
    
    # 2. Try default for project type
    if project_type and project_type in DEFAULT_PROMPTS:
        return DEFAULT_PROMPTS[project_type]
    
    # 3. Fallback to generic
    return DEFAULT_PROMPTS["generic"]

def parse_code_review_response(response: str) -> tuple[str, CodeReviewResult]:
    """
    Parse AI response that contains both commit message and code review.
    
    Args:
        response: Full AI response with commit message and optional review.
        
    Returns:
        Tuple of (commit_message, CodeReviewResult)
    """
    # Split on the code review marker
    marker = "---CODE_REVIEW---"
    
    if marker not in response:
        # No code review section, return original message
        return response.strip(), CodeReviewResult(has_issues=False)
    
    parts = response.split(marker, 1)
    commit_message = parts[0].strip()
    review_section = parts[1].strip() if len(parts) > 1 else ""
    
    # Parse the review section
    result = CodeReviewResult(has_issues=False)
    
    if not review_section or "OK" in review_section.upper()[:20]:
        result.summary = "Code looks clean."
        return commit_message, result
    
    # Parse issues
    issues = []
    type_mapping = {
        "SMELL": "code_smell",
        "BUG": "bug",
        "STYLE": "style",
        "PERF": "performance",
        "SECURITY": "security",
    }
    
    for line in review_section.split("\n"):
        line = line.strip()
        if not line or line.startswith("#"):
            continue
        
        # Try to parse issue format: - [TYPE] Description | Suggestion
        if line.startswith("-"):
            line = line[1:].strip()
        
        issue_type = "code_smell"
        severity = "warning"
        description = line
        suggestion = ""
        
        # Extract type
        for marker_type, mapped_type in type_mapping.items():
            if f"[{marker_type}]" in line.upper():
                issue_type = mapped_type
                # Remove the type marker
                description = line.upper().replace(f"[{marker_type}]", "").strip()
                description = line[line.upper().find("]") + 1:].strip() if "]" in line else line
                break
        
        # Set severity based on type
        if issue_type in ("bug", "security"):
            severity = "error"
        elif issue_type == "code_smell":
            severity = "warning"
        else:
            severity = "info"
        
        # Extract suggestion if present
        if "|" in description:
            parts = description.split("|", 1)
            description = parts[0].strip()
            suggestion = parts[1].strip()
        
        if description and len(description) > 3:
            issues.append(CodeIssue(
                type=issue_type,
                description=description,
                suggestion=suggestion,
                severity=severity,
            ))
    
    result.issues = issues
    result.has_issues = len(issues) > 0
    result.summary = f"Found {len(issues)} issue(s)" if issues else "Code looks clean."
    
    return commit_message, result


def format_review_for_display(result: CodeReviewResult, verbose: bool = False) -> str:
    """Format code review result for terminal display."""
    if not result.has_issues:
        return "âœ… Code review: No issues found."
    
    lines = [f"ðŸ“ Code review: {result.summary}"]
    
    severity_icons = {
        "info": "â„¹ï¸",
        "warning": "âš ï¸",
        "error": "âŒ",
    }
    
    for issue in result.issues:
        icon = severity_icons.get(issue.severity, "â€¢")
        lines.append(f"   {icon} [{issue.type}] {issue.description}")
        if verbose and issue.suggestion:
            lines.append(f"      ðŸ’¡ {issue.suggestion}")
    
    return "\n".join(lines)


def get_code_review_prompt_addon() -> str:
    """Get the prompt addon for code review."""
    return CODE_REVIEW_PROMPT_ADDON


def get_code_review_prompt() -> str:
    """Get the dedicated prompt for standalone code review."""
    return CODE_REVIEW_PROMPT


def parse_standalone_review(response: str) -> CodeReviewResult:
    """
    Parse AI response from standalone code review (no commit message).
    
    Args:
        response: AI response containing only code review.
        
    Returns:
        CodeReviewResult
    """
    response = response.strip()
    
    # Check for OK response (no issues)
    if response.upper().startswith("OK") or response.upper() == "OK":
        return CodeReviewResult(has_issues=False, summary="Code looks clean.")
    
    # Parse issues
    issues = []
    type_mapping = {
        "SMELL": "code_smell",
        "BUG": "bug",
        "STYLE": "style",
        "PERF": "performance",
        "SECURITY": "security",
    }
    
    for line in response.split("\n"):
        line = line.strip()
        if not line or line.startswith("#"):
            continue
        
        # Try to parse issue format: - [TYPE] Description | Suggestion
        if line.startswith("-"):
            line = line[1:].strip()
        
        issue_type = "code_smell"
        severity = "warning"
        description = line
        suggestion = ""
        
        # Extract type
        for marker_type, mapped_type in type_mapping.items():
            if f"[{marker_type}]" in line.upper():
                issue_type = mapped_type
                # Remove the type marker
                description = line[line.upper().find("]") + 1:].strip() if "]" in line else line
                break
        
        # Set severity based on type
        if issue_type in ("bug", "security"):
            severity = "error"
        elif issue_type == "code_smell":
            severity = "warning"
        else:
            severity = "info"
        
        # Extract suggestion if present
        if "|" in description:
            parts = description.split("|", 1)
            description = parts[0].strip()
            suggestion = parts[1].strip()
        
        if description and len(description) > 3:
            issues.append(CodeIssue(
                type=issue_type,
                description=description,
                suggestion=suggestion,
                severity=severity,
            ))
    
    return CodeReviewResult(
        has_issues=len(issues) > 0,
        issues=issues,
        summary=f"Found {len(issues)} issue(s)" if issues else "Code looks clean.",
    )
